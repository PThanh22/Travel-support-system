# Step1:  Nháº­p thÆ° viá»‡n
import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.llms import OpenAI
import openai
from llama_index import SimpleDirectoryReader


# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.vectorstores import Chroma
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.chains import ConversationalRetrievalChain
# from langchain.chat_models import ChatOpenAI
# from langchain.document_loaders import UnstructuredFileLoader
# import os

# import pandas as pd
# from langchain.chat_models import ChatOpenAI

# Step2: Khá»Ÿi táº¡o lá»‹ch sá»­ tin nháº¯n
''' 
+ Äáº·t khÃ³a API OpenAI cá»§a báº¡n tá»« bÃ­ máº­t cá»§a á»©ng dá»¥ng.
+ ThÃªm tiÃªu Ä‘á» cho á»©ng dá»¥ng cá»§a báº¡n.
+ Sá»­ dá»¥ng tráº¡ng thÃ¡i phiÃªn Ä‘á»ƒ theo dÃµi lá»‹ch sá»­ tin nháº¯n cá»§a chatbot.
+ Khá»Ÿi táº¡o giÃ¡ trá»‹ st.session_state.messagesÄ‘á»ƒ bao gá»“m thÃ´ng bÃ¡o báº¯t Ä‘áº§u cá»§a chatbot,
cháº³ng háº¡n nhÆ° "HÃ£y há»i tÃ´i má»™t cÃ¢u há»i vá» thÆ° viá»‡n Python nguá»“n má»Ÿ cá»§a Streamlit!" 
'''

openai.api_key = st.secrets.OPENAI_API_KEY
st.header("Chat with the Streamlit docs ğŸ’¬ ğŸ“š")

if "messages" not in st.session_state.keys(): # Khá»Ÿi táº¡o lá»‹ch sá»­ tin nháº¯n trÃ² chuyá»‡n
    st.session_state.messages = [
        {
            "role": "assistant", 
            "content": "Xin chÃ o? chÃºng tÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n? HÃ£y Ä‘á»ƒ láº¡i cÃ¢u há»i chÃºng tÃ´i sáº½ tráº£ lá»i báº¡n trong thá»i gian sá»›m nháº¥t ."
         }
    ]

# Step3: Táº£i vÃ  láº­p chá»‰ má»¥c dá»¯ liá»‡u
'''
+ LÆ°u trá»¯ cÃ¡c files CÆ¡ sá»Ÿ Kiáº¿n thá»©c cá»§a báº¡n trong má»™t thÆ° má»¥c cÃ³ tÃªn data trong á»©ng 
dá»¥ng. NhÆ°ng trÆ°á»›c khi báº¡n báº¯t Ä‘áº§uâ€¦

+ Táº£i xuá»‘ng cÃ¡c tá»‡p Ä‘Ã¡nh dáº¥u cho tÃ i liá»‡u cá»§a Streamlit tá»« data thÆ° má»¥c kho lÆ°u trá»¯ 
GitHub cá»§a á»©ng dá»¥ng demo. ThÃªm data folder vÃ o cáº¥p gá»‘c cá»§a á»©ng dá»¥ng cá»§a báº¡n.
NgoÃ i ra, hÃ£y thÃªm data cá»§a báº¡n.

+ XÃ¡c Ä‘á»‹nh má»™t hÃ m cÃ³ tÃªn load_data(), hÃ m nÃ y sáº½:
    - Sá»­ dá»¥ng LlamaIndex SimpleDirectoryReader Ä‘á»ƒ chuyá»ƒn LlamaIndex vÃ o folder
    nÆ¡i báº¡n Ä‘Ã£ lÆ°u trá»¯ data cá»§a mÃ¬nh (trong trÆ°á»ng há»£p nÃ y, nÃ³ Ä‘Æ°á»£c gá»i data vÃ  
    náº±m á»Ÿ cáº¥p cÆ¡ sá»Ÿ cá»§a kho lÆ°u trá»¯ cá»§a báº¡n).
    - SimpleDirectoryReader sáº½ chá»n trÃ¬nh Ä‘á»c files thÃ­ch há»£p dá»±a trÃªn pháº§n má»Ÿ rá»™ng 
    cá»§a files trong folder Ä‘Ã³ ( .md files cho vÃ­ dá»¥ nÃ y) vÃ  sáº½ táº£i táº¥t cáº£ cÃ¡c files 
    theo cÃ¡ch Ä‘á»‡ quy tá»« folders Ä‘Ã³ khi chÃºng tÃ´i gá»i reader.load_data().
    - XÃ¢y dá»±ng má»™t phiÃªn báº£n cá»§a LlamaIndex ServiceContext, pháº§n tÃ i nguyÃªn cá»§a 
    LlamaIndex Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c giai Ä‘oáº¡n láº­p chá»‰ má»¥c vÃ  truy váº¥n cá»§a Ä‘Æ°á»ng
    dáº«n RAG.
    - ServiceContext cho phÃ©p chÃºng tÃ´i Ä‘iá»u chá»‰nh cÃ¡c cÃ i Ä‘áº·t nhÆ° LLM vÃ  model nhÃºng
    Ä‘Æ°á»£c sá»­ dá»¥ng.
    - Sá»­ dá»¥ng LlamaIndex's VectorStoreIndex cho creaLlamaIndex'sory SimpleVectorStore, 
    viá»‡c nÃ y sáº½ cáº¥u trÃºc dá»¯ liá»‡u cá»§a báº¡n theo cÃ¡ch giÃºp model cá»§a báº¡n nhanh chÃ³ng 
    truy xuáº¥t ngá»¯ cáº£nh tá»« data cá»§a báº¡n. HÃ m nÃ y tráº£ vá» VectorStoreIndex Ä‘á»‘i tÆ°á»£ng.
    - Chá»©c nÄƒng nÃ y Ä‘Æ°á»£c gÃ³i trong bá»™ trang trÃ­ bá»™ nhá»› Ä‘á»‡m cá»§a Streamlit 
    st.cache_resource Ä‘á»ƒ giáº£m thiá»ƒu sá»‘ láº§n data Ä‘Æ°á»£c táº£i vÃ  láº­p chá»‰ má»¥c.

+ Cuá»‘i cÃ¹ng, gá»i hÃ m load_data, chá»‰ Ä‘á»‹nh VectorStoreIndex Ä‘á»‘i tÆ°á»£ng tráº£ vá» cá»§a nÃ³ sáº½
Ä‘Æ°á»£c gá»i index.
'''

@st.cache_resource(show_spinner=False)

def load_data():
    with st.spinner(text="Äang táº£i vÃ  láº­p chá»‰ má»¥c cÃ¡c tÃ i liá»‡u Streamlit â€“ hÃ£y chá» nhÃ©! QuÃ¡ trÃ¬nh nÃ y sáº½ máº¥t 1-2 phÃºt."):
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0.5, system_prompt="Báº¡n lÃ  chuyÃªn gia vá» thÆ° viá»‡n Streamlit Python vÃ  cÃ´ng viá»‡c cá»§a báº¡n lÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i ká»¹ thuáº­t. Giáº£ sá»­ ráº±ng táº¥t cáº£ cÃ¡c cÃ¢u há»i Ä‘á»u liÃªn quan Ä‘áº¿n thÆ° viá»‡n Streamlit Python. HÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i mang tÃ­nh ká»¹ thuáº­t vÃ  dá»±a trÃªn sá»± tháº­t â€“ Ä‘á»«ng táº¡o áº£o giÃ¡c vá» cÃ¡c tÃ­nh nÄƒng."))
        index = VectorStoreIndex.from_documents(docs, service_context=service_context)
        return index

index = load_data()

# Step4: Táº¡o cÃ´ng cá»¥ trÃ² chuyá»‡n
'''
+ LlamaIndex cung cáº¥p má»™t sá»‘ cháº¿ Ä‘á»™ khÃ¡c nhau cá»§a cÃ´ng cá»¥ trÃ² chuyá»‡n. CÃ³ thá»ƒ há»¯u Ã­ch
khi kiá»ƒm tra tá»«ng cháº¿ Ä‘á»™ báº±ng cÃ¡c cÃ¢u há»i dÃ nh riÃªng cho cÆ¡ sá»Ÿ kiáº¿n thá»©c vÃ  
trÆ°á»ng há»£p sá»­ dá»¥ng cá»§a báº¡n, so sÃ¡nh pháº£n há»“i do mÃ´ hÃ¬nh táº¡o ra trong tá»«ng cháº¿ Ä‘á»™.

+ LlamaIndex cÃ³ bá»‘n cÃ´ng cá»¥ trÃ² chuyá»‡n khÃ¡c nhau:
    - Condense question engine_CÃ´ng cá»¥ cÃ¢u há»i cÃ´ Ä‘á»ng : LuÃ´n truy váº¥n cÆ¡ sá»Ÿ kiáº¿n 
    thá»©c. CÃ³ thá»ƒ gáº·p ráº¯c rá»‘i vá»›i cÃ¡c cÃ¢u há»i meta nhÆ° â€œTrÆ°á»›c Ä‘Ã¢y tÃ´i Ä‘Ã£ há»i báº¡n 
    Ä‘iá»u gÃ¬?â€
    - Context chat engin_CÃ´ng cá»¥ trÃ² chuyá»‡n theo ngá»¯ cáº£nh" : LuÃ´n truy váº¥n cÆ¡ sá»Ÿ 
    kiáº¿n thá»©c vÃ  sá»­ dá»¥ng vÄƒn báº£n Ä‘Æ°á»£c truy xuáº¥t tá»« cÆ¡ sá»Ÿ kiáº¿n thá»©c lÃ m ngá»¯ cáº£nh 
    cho cÃ¡c truy váº¥n sau. Ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t tá»« cÃ¡c truy váº¥n trÆ°á»›c Ä‘Ã³ cÃ³ thá»ƒ 
    chiáº¿m nhiá»u ngá»¯ cáº£nh cÃ³ sáºµn cho truy váº¥n hiá»‡n táº¡i.
    - ReAct agent_TÃ¡c nhÃ¢n ReAct : Chá»n cÃ³ truy váº¥n cÆ¡ sá»Ÿ tri thá»©c hay khÃ´ng. 
    Hiá»‡u suáº¥t cá»§a nÃ³ phá»¥ thuá»™c nhiá»u hÆ¡n vÃ o cháº¥t lÆ°á»£ng cá»§a LLM. Báº¡n cÃ³ thá»ƒ cáº§n pháº£i
    buá»™c cÃ´ng cá»¥ trÃ² chuyá»‡n chá»n chÃ­nh xÃ¡c xem cÃ³ truy váº¥n cÆ¡ sá»Ÿ kiáº¿n thá»©c hay khÃ´ng.
    - OpenAI agent_TÃ¡c nhÃ¢n OpenAI : Chá»n cÃ³ truy váº¥n cÆ¡ sá»Ÿ kiáº¿n thá»©c hay khÃ´ng â€”
    tÆ°Æ¡ng tá»± nhÆ° cháº¿ Ä‘á»™ tÃ¡c nhÃ¢n ReAct, nhÆ°ng sá»­ dá»¥ng kháº£ nÄƒng truy váº¥n fuOpenAI's 
    tÃ­ch há»£p sáºµn cá»§a OpenAI .
    
BÃ i nÃ y sá»­ dá»¥ng cháº¿ Ä‘á»™ cÃ¢u há»i cÃ´ Ä‘á»ng vÃ¬ nÃ³ luÃ´n truy váº¥n cÆ¡ sá»Ÿ kiáº¿n thá»©c 
(cÃ¡c tá»‡p tá»« tÃ i liá»‡u Streamlit) khi táº¡o cÃ¢u tráº£ lá»i. Cháº¿ Ä‘á»™ nÃ y lÃ  tá»‘i Æ°u vÃ¬ 
báº¡n muá»‘n mÃ´ hÃ¬nh giá»¯ cÃ¡c cÃ¢u tráº£ lá»i cá»¥ thá»ƒ cho cÃ¡c tÃ­nh nÄƒng Ä‘Æ°á»£c Ä‘á» cáº­p 
trong tÃ i liá»‡u cá»§a Streamlit.
'''

chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)


# Step5: Nháº¯c ngÆ°á»i dÃ¹ng nháº­p vÃ  hiá»ƒn thá»‹ lá»‹ch sá»­ tin nháº¯n
'''
+ Sá»­ dá»¥ng tÃ­nh nÄƒng Streamlit's cá»§a st.chat_inputStreamlit Ä‘á»ƒ user nháº­p cÃ¢u há»i.
+ Khi user Ä‘Ã£ nháº­p thÃ´ng tin Ä‘áº§u vÃ o, hÃ£y thÃªm thÃ´ng tin Ä‘áº§u vÃ o Ä‘Ã³ vÃ o lá»‹ch sá»­ 
tin nháº¯n báº±ng cÃ¡ch thÃªm nÃ³ vÃ o st.session_state.messages.
+ Hiá»ƒn thá»‹ lá»‹ch sá»­ tin nháº¯n cá»§a chatbot báº±ng cÃ¡ch duyá»‡t qua ná»™i dung Ä‘Æ°á»£c liÃªn káº¿t
vá»›i khÃ³a â€œmessagesâ€ á»Ÿ tráº¡ng thÃ¡i phiÃªn vÃ  hiá»ƒn thá»‹ tá»«ng message báº±ng st.chat_message.
'''

if prompt := st.chat_input("Your question"): # Nháº¯c user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Hiá»ƒn thá»‹ tin nháº¯n trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³
    with st.chat_message(message["role"]):
        st.write(message["content"])

# Step6: Chuyá»ƒn truy váº¥n tá»›i cÃ´ng cá»¥ trÃ² chuyá»‡n vÃ  hiá»ƒn thá»‹ pháº£n há»“i
'''
Náº¿u tin nháº¯n cuá»‘i cÃ¹ng trong lá»‹ch sá»­ tin nháº¯n khÃ´ng pháº£i tá»« chatbot, hÃ£y chuyá»ƒn 
ná»™i dung tin nháº¯n Ä‘áº¿n cÃ´ng cá»¥ trÃ² chuyá»‡n thÃ´ng qua chat_engine.chat(), viáº¿t pháº£n 
há»“i cho giao diá»‡n user báº±ng cÃ¡ch sá»­ dá»¥ng st.write vÃ  st.chat_message, Ä‘á»“ng thá»i 
thÃªm pháº£n há»“i cá»§a cÃ´ng cá»¥ trÃ² chuyá»‡n vÃ o lá»‹ch sá»­ tin nháº¯n.
'''

# Náº¿u tin nháº¯n cuá»‘i cÃ¹ng khÃ´ng pháº£i tá»« trá»£ lÃ½ [assistant], hÃ£y táº¡o pháº£n há»“i má»›i
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # ThÃªm pháº£n há»“i vÃ o lá»‹ch sá»­ tin nháº¯n
            
            
#Step7: Triá»ƒn khai á»©ng dá»¥ng!
'''
+ Sau khi xÃ¢y dá»±ng á»©ng dá»¥ng, hÃ£y triá»ƒn khai nÃ³ trÃªn Streamlit Community Cloud:
    - Táº¡o kho lÆ°u trá»¯ GitHub.
    - Äiá»u hÆ°á»›ng Ä‘áº¿n Streamlit Community Cloud , nháº¥p vÃ o New appvÃ  chá»n kho 
    lÆ°u trá»¯, nhÃ¡nh vÃ  Ä‘Æ°á»ng dáº«n tá»‡p thÃ­ch há»£p.
    - ÄÃ¡nh Deploy.
    
+ Káº¿t quáº£: Báº¡n cÅ©ng Ä‘Ã£ xÃ¢y dá»±ng má»™t á»©ng dá»¥ng chatbot sá»­ dá»¥ng LlamaIndex Ä‘á»ƒ tÄƒng c
Æ°á»ng GPT-3.5 trong 43 dÃ²ng mÃ£. TÃ i liá»‡u Streamlit cÃ³ thá»ƒ Ä‘Æ°á»£c thay tháº¿ cho báº¥t ká»³ 
nguá»“n dá»¯ liá»‡u tÃ¹y chá»‰nh nÃ o. Káº¿t quáº£ lÃ  má»™t á»©ng dá»¥ng mang láº¡i cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c
vÃ  cáº­p nháº­t hÆ¡n nhiá»u cho cÃ¡c cÃ¢u há»i vá» thÆ° viá»‡n Python nguá»“n má»Ÿ Streamlit so vá»›i
ChatGPT hoáº·c chá»‰ sá»­ dá»¥ng GPT.
'''

# Äá»c dá»¯ liá»‡u tá»« táº­p tin CSV

def load_data(file_path):
    return pd.read_csv(file_path)

file_path = "your_dataset.csv"
data = load_data(file_path)

# Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (náº¿u cáº§n)
# VÃ­ dá»¥: loáº¡i bá» cÃ¡c dÃ²ng trá»‘ng
data.dropna(inplace=True)

# Huáº¥n luyá»‡n chatbot
llm = ChatOpenAI(temperature=0, max_tokens=1000, model_name="gpt-3.5-turbo", streaming=True)

# Huáº¥n luyá»‡n chatbot dá»±a trÃªn dá»¯ liá»‡u tá»« táº­p tin CSV
for row in data.itertuples(index=False):
    user_input = row.question  # Giáº£ sá»­ cá»™t "question" trong táº­p dá»¯ liá»‡u chá»©a cÃ¡c cÃ¢u há»i
    response = row.answer  # Giáº£ sá»­ cá»™t "answer" trong táº­p dá»¯ liá»‡u chá»©a cÃ¡c cÃ¢u tráº£ lá»i

    # Huáº¥n luyá»‡n chatbot vá»›i má»—i cáº·p cÃ¢u há»i - cÃ¢u tráº£ lá»i tá»« táº­p dá»¯ liá»‡u
    llm.train(user_input, response)

# XÃ¢y dá»±ng giao diá»‡n ngÆ°á»i dÃ¹ng
st.title("Simple Chatbot with Streamlit")

user_input = st.text_input("You:", "")

if user_input:
    # Gá»­i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Ä‘áº¿n chatbot
    response = llm.ask(user_input)

    st.text_area("Bot:", value=response, height=200)


